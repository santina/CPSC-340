{
    "contents" : "# the following code are from connect, and has been heavily commented by me. \n\ntrain<- read.table(\"admission-train.csv\", sep = \",\", header = TRUE)\ntest <- read.table(\"admission-test.csv\", sep = \",\", header = TRUE)\nhead(train)\nsummary(train)\n#contingency table : makes sense more for categorial data\nxtabs(~admit + rank, data = train) # categorical \nxtabs(~admit + gre, data = train)  # numerical (gre), doesn't make as much sense\n\n#convert numerical to categorical\ntrain$rank <- factor(train$rank)\nhead(train$rank)\ntest$rank <- factor(test$rank)\ntrain$admit <- factor(train$admit)\ntest$admit <- factor(test$admit)\n\n# training the first logistic regression model (glm for logistic regression)\nmodel1 <- glm(admit ~ gre + gpa, data = train, family=\"binomial\")\nsummary(model1) \n# P(gre, gpa) = (1 - e^-(-3.37+0.00266*gre + 0.278*gpa))^-1\n# P(gre) = (1+e^(-b_0 + B_1*gre))^-1  a sigmoid function: gre better, closer to 1\n# number of stars indicates how significantly influential they're\n\n\npredict1 <- predict(model1, newdata = test, type = \"response\")\n#look at the precict1 values and guess what are the values? \npredict1  # there are 100 cases \n# in order to change the probabilities to binary decision, we need to find a\n# cutoff. We want a cutoff that gives us the best both sensitivity/specifity.\n# wanna see how sensitivity and specificity change when we change this cutoff \n# gonna try <0.5 didn't get in, and >=0.5 gets in\n\nperf = function(cut, mod, y)\n   {\n        yhat = (mod$fit>cut)\n        w = which(y==1)\n        sensitivity = mean( yhat[w] == 1 ) \n        specificity = mean( yhat[-w] == 0 ) \n        out = t(as.matrix(c(sensitivity, specificity)))\n        colnames(out) = c(\"sensitivity\", \"specificity\")\n        return(out)\n     }\n\nperf(0.1,model1,train$admit) # completely sensitive but not specific \nperf(0.5,model1,train$admit)  \nperf(0.8,model1,train$admit)\n\nsensetivity=c()\nspecifity=c()\n\nfor(i in seq(0.1, 0.9, by=0.1)){\n  sensetivity <- c(sensetivity,perf(i,model1,train$admit)[1])\n  specifity <- c(specifity,perf(i,model1,train$admit)[2])\n  }\n\n#find the cutoff\nplot(seq(0.1, 0.9, by=0.1), sensetivity , type=\"l\", lty=1, col=\"blue\", \n     xlab=\"cutoff points\", ylab=\"sensetivity/specifity\", lwd=2)\nlines(seq(0.1, 0.9, by=0.1),specifity, type=\"l\", lty=1, col=\"red\", lwd=2)\n# best cutoff at intercept \n\n#prediction\npredict1 <- ifelse(predict(model1, newdata=test,type=\"response\")>0.3, 1, 0)\n# if prob greater than 0.3, we san it's 1 otherwise 0\n\nconfusionmatrix1 <- table(predict1, test$admit)\nconfusionmatrix1\n#accuracy of model over the test data:\nsum(diag(confusionmatrix1))/sum(confusionmatrix1)\n\n\n\n##### model 2:\nmodel2 <- glm(admit ~ gre + gpa +  rank, data = train, family=\"binomial\")\nsummary(model2)\nsensetivity=c()\nspecifity=c()\nfor(i in seq(0.1, 0.9, by=0.1)){\n  sensetivity <- c(sensetivity,perf(i,model2,train$admit)[1])\n  specifity <- c(specifity,perf(i,model2,train$admit)[2])\n}\n\n\n\n#find the cutoff\nplot(seq(0.1, 0.9, by=0.1), sensetivity , type=\"l\", lty=1, col=\"blue\", xlab=\"cutoff points\", \n     ylab=\"sensetivity/specifity\", lwd=2)\nlines(seq(0.1, 0.9, by=0.1),specifity, type=\"l\", lty=1, col=\"red\", lwd=2)\n\n#prediction\npredict2 <- ifelse(predict(model2, newdata=test,type=\"response\")>0.3, 1, 0)\nconfusionmatrix2 <- table(predict2, test$admit)\nconfusionmatrix2\n\n# columns \n\n#accuracy of model over the test data:\nsum(diag(confusionmatrix2))/sum(confusionmatrix2)\n\n# 0.66 ... better than model 1 by 2% \n\n\n\n",
    "created" : 1425522340149.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2139875306",
    "id" : "FB46B037",
    "lastKnownWriteTime" : 1413862492,
    "path" : "C:/Users/Santina Lin/SkyDrive/CPSC340/Tutorials/Tutorial4/Tutorial-4-scripts.R",
    "project_path" : "Tutorials/Tutorial4/Tutorial-4-scripts.R",
    "properties" : {
    },
    "source_on_save" : false,
    "type" : "r_source"
}