library(rpart) #need this for classification
train<- read.table("epinions-train.csv", sep = ",", header = TRUE)
test<- read.table("epinions-test.csv", sep = ",", header = TRUE)
set.seed(123) #ensure reproducibility
data_set  <- train[sample(nrow(train), 150), ] #randomy select 150 items from the training dataset
test_set  <- data_set[100:150,] #same as rain_test_set  <-  data_set, 50 items for testing
train_set  <- data_set[1:100,] # get 100 items to train the model
train_test_set <- rbind(train_set,test_set)
train_test_corpus <- Corpus(VectorSource(train_test_set$text))
train_test_corpus <- tm_map(train_test_corpus, content_transformer(tolower))
train_test_corpus <- tm_map(train_test_corpus, content_transformer(removePunctuation))
train_test_corpus <- tm_map(train_test_corpus, content_transformer(removeWords) , c(stopwords('english')))
#train_test_corpus[[1]] ##checking to see if words are removed
train_test_corpus <- tm_map(train_test_corpus, stripWhitespace) #spaces are removed too
DTM_train_test <- DocumentTermMatrix(train_test_corpus,control=list(stemming = TRUE))
DTM_train_test  #sparsity: 96%, non-spase/spase entries: 24K versus 66K
train_test_dataframe <- as.data.frame(inspect( DTM_train_test ))
#we basically transformed the original matrix to one with column of classes
class <- c(train_test_set$class)
train_test_df_withclass <- cbind( train_test_dataframe, classification)
train_test_df_withclass <- cbind( train_test_dataframe, class)
train_df_withclass <- head (train_test_df_withclass,100)
test_df_withclass <- tail (train_test_df_withclass,50)
dt_model <- rpart(classification ~ .,train_df_withclass,method="class")
dt_model <- rpart(class ~ .,train_df_withclass,method="class")
rm(list=ls())
#install.packages("SnowballC")
#install.packages("rpart")
library(SnowballC)
library(tm)
library(rpart) #need this for classification
train<- read.table("epinions-train.csv", sep = ",", header = TRUE)
test<- read.table("epinions-test.csv", sep = ",", header = TRUE)
#building the training and test set
set.seed(123) #ensure reproducibility
data_set  <- train[sample(nrow(train), 150), ] #randomy select 150 items from the training dataset
test_set  <- data_set[100:150,] #same as rain_test_set  <-  data_set, 50 items for testing
train_set  <- data_set[1:100,] # get 100 items to train the model
#train_set <- train[sample(nrow(train), 100),]
#test_set <- test[sample(nrow(test), 50),]
#now we combine the two datasets together
train_test_set <- rbind(train_set,test_set)
#train_test_set$class   #basically you have two components in this dataframe
#train_test_set$text
#goal to predict via decision tree classifer whether given doc is about auto or camera
#turn the data frame's text component into a corpus
train_test_corpus <- Corpus(VectorSource(train_test_set$text))
#preprocess the corpus (content transformer is used to remove some error message whch idk why happened)
train_test_corpus <- tm_map(train_test_corpus, content_transformer(tolower))
train_test_corpus <- tm_map(train_test_corpus, content_transformer(removePunctuation))
train_test_corpus <- tm_map(train_test_corpus, content_transformer(removeWords) , c(stopwords('english')))
#train_test_corpus[[1]] ##checking to see if words are removed
train_test_corpus <- tm_map(train_test_corpus, stripWhitespace) #spaces are removed too
#train_test_corpus[[1]]
# steamming helps things like "say" and "says" are the same (only stem "say" is kept)
# DTM stands for?
DTM_train_test <- DocumentTermMatrix(train_test_corpus,control=list(stemming = TRUE))
DTM_train_test  #sparsity: 96%, non-spase/spase entries: 24K versus 66K
#for comparison purpose, below code would return 97% sparsity, 26K/938K
#b  <-  DocumentTermMatrix(train_test_corpus)
train_test_dataframe <- as.data.frame(inspect( DTM_train_test ))
#we basically transformed the original matrix to one with column of classes
categories <- c(train_test_set$class)
train_test_df_withclass <- cbind( train_test_dataframe, categories)
#now we separate them again
train_df_withclass <- head (train_test_df_withclass,100)
test_df_withclass <- tail (train_test_df_withclass,50)
dt_model <- rpart(classification ~ .,train_df_withclass,method="class")
dt_model <- rpart(categories ~ .,train_df_withclass,method="class")
dt_model
prediction <- predict(dt_model, test_df_withclass, type="class")
dt_table  <- table (test_df_withclass$classification,prediction)#this is the confusion matrix
rm(list=ls())
library(SnowballC)
library(tm)
library(rpart) #need this for classification
train<- read.table("epinions-train.csv", sep = ",", header = TRUE)
test<- read.table("epinions-test.csv", sep = ",", header = TRUE)
#building the training and test set
set.seed(123) #ensure reproducibility
data_set  <- train[sample(nrow(train), 150), ] #randomy select 150 items from the training dataset
test_set  <- data_set[100:150,] #same as rain_test_set  <-  data_set, 50 items for testing
train_set  <- data_set[1:100,] # get 100 items to train the model
#train_set <- train[sample(nrow(train), 100),]
#test_set <- test[sample(nrow(test), 50),]
#now we combine the two datasets together
train_test_set <- rbind(train_set,test_set)
#train_test_set$class   #basically you have two components in this dataframe
#train_test_set$text
#goal to predict via decision tree classifer whether given doc is about auto or camera
#turn the data frame's text component into a corpus
train_test_corpus <- Corpus(VectorSource(train_test_set$text))
#preprocess the corpus (content transformer is used to remove some error message whch idk why happened)
train_test_corpus <- tm_map(train_test_corpus, content_transformer(tolower))
train_test_corpus <- tm_map(train_test_corpus, content_transformer(removePunctuation))
train_test_corpus <- tm_map(train_test_corpus, content_transformer(removeWords) , c(stopwords('english')))
#train_test_corpus[[1]] ##checking to see if words are removed
train_test_corpus <- tm_map(train_test_corpus, stripWhitespace) #spaces are removed too
#train_test_corpus[[1]]
# steamming helps things like "say" and "says" are the same (only stem "say" is kept)
# DTM stands for?
DTM_train_test <- DocumentTermMatrix(train_test_corpus,control=list(stemming = TRUE))
DTM_train_test  #sparsity: 96%, non-spase/spase entries: 24K versus 66K
#for comparison purpose, below code would return 97% sparsity, 26K/938K
#b  <-  DocumentTermMatrix(train_test_corpus)
train_test_dataframe <- as.data.frame(inspect( DTM_train_test ))
#we basically transformed the original matrix to one with column of classes
classification <- c(train_test_set$class)
train_test_df_withclass <- cbind( train_test_dataframe, classification)
#now we separate them again
train_df_withclass <- head (train_test_df_withclass,100)
test_df_withclass <- tail (train_test_df_withclass,50)
# . means include all the terms in making the model
dt_model <- rpart(classification ~ .,train_df_withcl
prediction <- predict(dt_model, test_df_withclass, type="class")
dt_model <- rpart(classification ~ .,train_df_withclass,method="class")
prediction <- predict(dt_model, test_df_withclass, type="class")
dt_table  <- table (test_df_withclass$classification,prediction)#this is the confusion matrix
dt_table
printcp(dt_model)
plot(fit, uniform=TRUE, main="glah")
plot(dt_model, uniform=TRUE, main="glah")
setwd("C:/Users/Santina Lin/Desktop/CPSC340/Assignments_cs340/Assignment2_Oct15")
files.list()
list.files()
trainingData<- read.table("2014CensusTraining.csv", sep = ",", header = TRUE)
head(trainingData)
summary(trainingData)
trainingData[1]
trainingData[,1]
trainingData[1, ]
class(trainingData)
?data.frame
c  <- colnames(trainingData)
c
install.packages("rpart.plot")
names(trainingData)
colnames(trainingDAta)
colnames(trainingData)
predictors <- colnames(trainingData)
model_F5 <- rpart(class ~ predictors[1:5],trainingData,method="class")
?rpart
model_F5 <- rpart(class ~ predictors[1:5],trainingData,method="class")
model_F5 <- rpart(class ~ %in% predictors[1:5],trainingData,method="class")
model_F5 <- rpart(class ~ p %in% predictors[1:5],trainingData,method="class")
names(trainingData)
predictors <- names(trainingData)
predictors
predictors[1:5]
model_F5 <- rpart(class ~ predictors[1:5],trainingData,method="class")
model_F5 <- rpart(class ~ (predictors[1:5]),trainingData,method="class")
model_F5 <- rpart(class ~ p = predictors[1:5],trainingData,method="class")
model_F5 <- rpart(class ~ workclass + age + fnlwgt + education + education.num,trainingData,method="class")
predictors <- names(trainingData)
model_F5 <- rpart(class ~ predictors[1:5],trainingData,method="class")
model_F5 <- rpart(class ~ "workclass + age + fnlwgt + education + education.num",trainingData,method="class")
?as.factor()
model_F5 <- rpart(class ~ as.factor(predictors[1:5]),trainingData,method="class")
model_F5 <- rpart(class ~ predictors[1],trainingData,method="class")
predictors <- names(trainingData)
model_F5 <- rpart(class ~ workclass + age + fnlwgt + education +
education.num,trainingData,method="class")
F5 <- predict(model_F5, trainingData, type="class")
#this is the confusion matrix
dt_table  <- table (test_df_withclass$classification,prediction)
dt_table
rm(list=ls())
library(SnowballC)
library(tm)
library(rpart) #need this for classification, decision tree
library(rpart.plot) # to make pretty tree, thanks to Jonathan Stiansen's suggestion
trainingData<- read.table("2014CensusTraining.csv", sep = ",", header = TRUE)
#=======2. Make a decision tree F5 with first 5 attributes==========
# . two classes are people making >50K or less, column name : class
predictors <- names(trainingData)
model_F5 <- rpart(class ~ workclass + age + fnlwgt + education +
education.num,trainingData,method="class")
F5 <- predict(model_F5, trainingData, type="class")
dt_table  <- table (data$class, F5)
dt_table  <- table (trainingdata$class, F5)
library(SnowballC)
library(tm)
library(rpart) #need this for classification, decision tree
library(rpart.plot) # to make pretty tree, thanks to Jonathan Stiansen's suggestion
#=======1. Load census training dataset ============================
#training data, full and 50%
trainingData<- read.table("2014CensusTraining.csv", sep = ",", header = TRUE)
#=======2. Make a decision tree F5 with first 5 attributes==========
# . two classes are people making >50K or less, column name : class
predictors <- names(trainingData)
model_F5 <- rpart(class ~ workclass + age + fnlwgt + education +
education.num,trainingData,method="class")
F5 <- predict(model_F5, trainingData, type="class")
dt_table  <- table (trainingData$class, F5)
dt_table
?table
table (trainingData$class)
dt_table[1,2]
sum(dt_table[1,])
inspect(dt_table)
?inspect
matrix_F5  <- table (trainingData$class, F5)
matrix_F5  <- mutate(dt_table, c(sum(matrix_F5[1,]), sum(matrix_F5[2,])))
library(dplyr)
matrix_F5  <- mutate(dt_table, c(sum(matrix_F5[1,]), sum(matrix_F5[2,])))
matrix_F5  <- table (trainingData$class, F5) %>% as.data.frame()
matrix_F5  <- mutate(dt_table, c(sum(matrix_F5[1,]), sum(matrix_F5[2,])))
class(matrix_F5)
matrix_F5  <- mutate(dt_table, c(sum(matrix_F5[1,]), sum(matrix_F5[2,])))
matrix_F5  <- mutate(matrix_F5, c(sum(matrix_F5[1,]), sum(matrix_F5[2,])))
sum(matrix_F5'')
sum(matrix_F5[1,])
?table
matrix_F5  <- table (trainingData$class, F5) )
matrix_F5  <- table (trainingData$class, F5)
tally(matrix_F5)
tally(as.data.frame(matrix_F5))
summarize(as.data.frame(matrix_F5))
sum(as.data.frame(matrix_F5))
f  <- as.data.frame(matrix_F5)
f
prp(F5)
prp(model_F5)
matrix_F5
?table
d  <- rbind(matrix_F5, c("3", "4"))
d
rm(d)
d  <- rbind(matrix_F5, c(sum(matrix_F5[,1]), sum(matrix_F5[,2])))
d
rownames(d)  <- c("<=50K", ">50K", "sum")
d
source('./helperFuns_A2_SantinaLin.R')
source('./helperFuns_A2_SantinaLin.R')
d <- summarize_matrix(matrix_F5)
d <- summarizeMatrix(matrix_F5)
model_F5 <- rpart(class ~ workclass + age + fnlwgt + education +
education.num,trainingData,method="class")
F5 <- predict(model_F5, trainingData, type="class")
prp(model_F5)
plot(model_F5)
test(decision.tree, all = TRUE)
test(model_F5, all = TRUE)
text(model_F5, all = TRUE)
tree_F5 <- rpart(class ~ workclass + age + fnlwgt + education +
education.num,trainingData,method="class")
prp(tree_F5) #plot a pretty tree
F5 <- predict(tree_F5, trainingData, type="class")
#this is the confusion matrix
matrix_F5  <- table (trainingData$class, F5)
matrix_F5
d <- summarizeMatrix(matrix_F5)
d  <- rbind(matrix_F5, c(sum(matrix_F5[,1]), sum(matrix_F5[,2])))
d
d  <- cbind(matrix_F5, c(sum(matrix_F5[1,]), sum(matrix_F5[2,])))
rownames(d) <- c("<=50K", ">50K" , "sum")
rownames(d)
d
d <- summarizeMatrix(matrix_F5)
d  <- cbind(matrix_F5, c(sum(matrix_F5[1,]), sum(matrix_F5[2,])))
d
d  <- rbind(matrix_F5, c(sum(matrix_F5[,1]), sum(matrix_F5[,2])))
d
rownames(d) <- c("<=50K", ">50K" , "sum")
d
d  <- cbind(matrix_F5, c(sum(matrix_F5[1,]), sum(matrix_F5[2,])))
d
d  <- cbind(matrix_F5, c(sum(matrix_F5[1,]), sum(matrix_F5[2,]), sum(matrix_F5[3,])))
d
d <- summarizeMatrix(matrix_F5)
d  <- rbind(matrix_F5, c(sum(matrix_F5[,1]), sum(matrix_F5[,2])))
d
rownames(d) <- c("<=50K", ">50K" , "sum")
d
d  <- cbind(matrix_F5, c(sum(matrix_F5[1,]), sum(matrix_F5[2,]), sum(matrix_F5[3,])))
d
colnames(d) <- c("<=50K", ">50K" , "sum")
d
d <- summarizeMatrix(matrix_F5)
matrix_F5  <- table (trainingData$class, F5)
matrix_F5
d <- summarizeMatrix(matrix_F5)
source('C:/Users/Santina Lin/Desktop/CPSC340/Assignments_cs340/Assignment2_Oct15/helperFuns_A2_SantinaLin.R')
rownames(matrix_F5)
d <- summarizeMatrix(matrix_F5)
d  <- rbind(matrix_F5, c(sum(matrix_F5[,1]), sum(matrix_F5[,2])))
d
d  <- cbind(d, c(sum(d[1,]), sum(d[2,]), sum(d[3,])))
d
rownames(d) <- c("<=50K", ">50K" , "sum")
colnames(d) <- c("<=50K Predicted", ">50K Predicted" , "total")
d
d <- summarizeMatrix(matrix_F5)
rm(list=ls())
library(SnowballC)
library(dplyr)
library(rpart) #need this for classification, decision tree
library(rpart.plot) # to make pretty tree, thanks to Jonathan Stiansen's suggestion
source('./helperFuns_A2_SantinaLin.R')
trainingData<- read.table("2014CensusTraining.csv", sep = ",", header = TRUE)
predictors <- names(trainingData)
tree_F5 <- rpart(class ~ workclass + age + fnlwgt + education +
education.num,trainingData,method="class")
prp(tree_F5) #plot a pretty tree
F5 <- predict(tree_F5, trainingData, type="class")
matrix_F5  <- table (trainingData$class, F5)
matrix_F5
d <- summarizeMatrix(matrix_F5)
d
trainingData_F10 <- trainingData[,c(1:10,15)]
#make and plot a pretty tree
tree_F10 <- rpart(class ~ . ,trainingData_F10,method="class")
prp(tree_F10)
tree_F14 <- rpart(class ~ . ,trainingData,method="class")
prp(tree_F14)
F5 <- predict(tree_F5, testingData, type="class")
testingData<- read.table("1014NewCensusTest.csv", sep = ",", header = TRUE)
setwd("C:/Users/Santina Lin/Desktop/CPSC340/Assignments_cs340/Assignment2_Oct15")
testingData<- read.table("1014NewCensusTest.csv", sep = ",", header = TRUE)
files.list()
list.files()
testingData<- read.table("2014NewCensusTest.csv", sep = ",", header = TRUE)
F5 <- predict(tree_F5, testingData, type="class")
matrix_F5  <- table (trainingData$class, F5) %>% summarizeMatrix()
matrix_F5
matrix_F5  <- table (trainingData$class, F5)
F5 <- predict(tree_F5, testingData, type="class")
matrix_F5  <- table (trainingData$class, F5)
F5
F5 <- predict(tree_F5, testingData, type="class")
matrix_F5  <- table (testingData$class, F5)
matrix_F5  <- table (testingData$class, F5)
matrix_F5  <- table (testingData$class, F5) %>% summarizeMatrix()
matrix_F5
matrix_F5[1,1]
prp(tree_F5)
matrix_F5[1,2]
sensitivity  <- matrix_F5[1,1]/(matrix_F5[1,1]+matrix_F5[1,2])
sensitivity
F5_sen  <- computeSensitivity(matrix_F5)
source('./helperFuns_A2_SantinaLin.R')
F5_sen  <- computeSensitivity(matrix_F5)
F5_sen
F5_spe <- computerSpecificity(matrix_F5)
F5_spe
source('./helperFuns_A2_SantinaLin.R')
compute_Spec_Sen(matrix_F5)
source('./helperFuns_A2_SantinaLin.R')
ans <- compute_Spec_Sen(matrix_F5)
ans
predictionQuality <- data.frame()
class(predictionQuality)
predictionQuality
source('./helperFuns_A2_SantinaLin.R')
compute_Spec_Sen(matrix_F5)
list  <- c(F5, F10)
trainingData_F10 <- trainingData[,c(1:10,15)]
#make and plot a pretty tree
tree_F10 <- rpart(class ~ . ,trainingData_F10,method="class")
list  <- c(F5, F10)
c(tree_F5, tree_F10)
list <- c(tree_F5, tree_F10)
l <- c("blah", "adsf", "agdf")
a  <- data.frame()
for(i in l){l + "ablksdj"}
for(i in l){l.append("ablksdj")}
rm(a)
rm(l)
rm(list)
answer <- compute_Spec_Sen(matrix_F5)
answer
answer[1,]
answer[,1]
frame <- data.frame()
frame
frame  <- rbind(frame, answer)
frame
frame  <- rbind(frame, answer)
frame
class(F5)
class(F10)
c(F5, F5)
name  <- c(F5, F5)
deparse(substitute(name))
name  <- deparse(substitute(name))
name
name  <- c(F5, F5)
name
name  <- deparse(substitute(name))
name
name  <- c(F5, F5)
name  <- deparse(name)
name
source('./helperFuns_A2_SantinaLin.R')
source('./helperFuns_A2_SantinaLin.R')
predictionQuality <- data.frame()
trees <- c(tree_F5, tree_F10, tree_F14, tree_H5, tree_H10, tree_H14)
rm(list=ls())
library(SnowballC)
library(dplyr)
library(rpart) #need this for classification, decision tree
library(rpart.plot) # to make pretty tree, thanks to Jonathan Stiansen's suggestion
#for calculating TP and TN and making the confusion matrix more clear
source('./helperFuns_A2_SantinaLin.R')
#=======1. Load census training dataset ============================
#training data, full and 50%
trainingData<- read.table("2014CensusTraining.csv", sep = ",", header = TRUE)
#=======2. Make a decision tree F5 with first 5 attributes==========
# . two classes are people making >50K or less, column name : class
#predictors #to see the predictors we can work with
predictors <- names(trainingData)
#create decision tree
tree_F5 <- rpart(class ~ workclass + age + fnlwgt + education +
education.num,trainingData,method="class")
prp(tree_F5) #plot a pretty tree
#=======3. Make a decision tree F10 with first 10 attributes========
#first 10 columns (predictors) and the 15th (class)
trainingData_F10 <- trainingData[,c(1:10,15)]
#make and plot a pretty tree
tree_F10 <- rpart(class ~ . ,trainingData_F10,method="class")
prp(tree_F10)
#=======4. Make a decision tree F14 with all 14 attributes==========
# . means include all the terms in making the model
tree_F14 <- rpart(class ~ . ,trainingData,method="class")
prp(tree_F14)
#=======5. Make a decision tree H5, H10, H14 with half data ========
training50  <- read.table("2014HalfCensusTraining.csv", sep = ",", header = TRUE)
#H5
tree_H5 <- rpart(class ~ workclass + age + fnlwgt + education +
education.num,training50,method="class")
#H10
training50_H10 <- trainingData[,c(1:10,15)]
tree_H10 <- rpart(class ~ . ,training50_H10,method="class")
#H14
tree_H14 <- rpart(class ~ . ,training50,method="class")
#=======6. Load the census test set=================================
#testing data
testingData <- read.table("2014NewCensusTest.csv", sep = ",", header = TRUE)
#=======7. Compute sensitivity and psecificty for all trees ========
# I'm defining positive as people who full under <=50K category
# Gonna store everything in one data frame
predictionQuality <- data.frame()
trees <- c(tree_F5, tree_F10, tree_F14, tree_H5, tree_H10, tree_H14)
names <- c("F5", "F10", "F14", "H5", "H10", "H14")
computeAllQualities(trees, testingData, names)
length(trees)
trees <- c(tree_F5, tree_F10, tree_F14, tree_H5, tree_H10, tree_H14)
length(trees)
trees.length()
trees[1]
trees[2]
trees[3]
trees[7]
trees[8]
trees[[7]
]
trees[[1]]
trees <- list(tree_F5, tree_F10, tree_F14, tree_H5, tree_H10, tree_H14)
trees[1]
trees[2]
source('./helperFuns_A2_SantinaLin.R')
source('./helperFuns_A2_SantinaLin.R')
source('./helperFuns_A2_SantinaLin.R')
computeAllQualities(trees, testingData, names)
length(trees)
#F5:
source('./helperFuns_A2_SantinaLin.R')
computeAllQualities(trees, testingData, names)
source('./helperFuns_A2_SantinaLin.R')
computeAllQualities(trees, testingData, names)
library(ggplot2)
ggplot(data=predictionQuality, aes(y=specificity + sensitivity)) +
geom_histogram(stat="identity", alpha=0.5)
mdata <- melt(predictionQuality, id=c("type","score"))
library(reshape)
install.packages(reshape)
install.packages("reshape")
library(reshape)
mdata <- melt(predictionQuality, id=c("type","score"))
predictionQuality
predictionQuality <-  computeAllQualities(trees, testingData, names)
predictionQuality
mdata <- melt(predictionQuality, id=c("type","score"))
?mutate
p <- predictionQuality
mutate(p, tree=rownames(p))
mdata <- melt(predictionQuality, id=c("tree","score"))
mdata <- melt(predictionQuality, id=c("tree"))
mdata <- melt(predictionQuality, id="tree")
p <- melt(p, id="tree")
class(p)
p
p <- mutate(p, tree=rownames(p))
p <- melt(p, id="tree")
p
ggplot(data=p, aes(x=tree, y=value, fill=variable)) +
geom_histogram(stat="identity", alpha=0.5)
ggplot(data=p, aes(x=tree, y=value, fill=variable)) +
geom_bar(stat="identity", position=position_dodge(), colour="black")
p <- mutate(predictionQuality, tree=rownames(predictionQuality))
predictionQuality
p
```{r, echo=FALSE, message=FALSE}
tree_F5 <- rpart(class ~ workclass + age + fnlwgt + education +
education.num,trainingData,method="class")
prediction <- predict(tree_F5, testingData, type="class")
matrix  <- table (testingData$class, prediction) %>% summarizeMatrix()
matrix
matrix  <- table (testingData$class, prediction)
matrix
62/89
78/91
